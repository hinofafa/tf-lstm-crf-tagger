{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmCrfTagger(object):\n",
    "    START_TAG = '<START>'\n",
    "    END_TAG   = '<END>'\n",
    "    @staticmethod\n",
    "    def log_sum_exp(vec):\n",
    "        max_score = tf.reduce_max(vec, axis=1)\n",
    "        return max_score + tf.log(tf.reduce_sum(tf.exp(vec - tf.expand_dims(max_score, axis=1)), axis=1))\n",
    "    @property\n",
    "    def tag_size(self): return len(self._tag_to_ix)    \n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim=5, hidden_dim=4):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._tag_to_ix = tag_to_ix\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._hidden_dim = hidden_dim\n",
    "    def _make_graph(self, graph):\n",
    "        with graph.as_default():\n",
    "            sentence = tf.placeholder(tf.int32, shape=[None], name='sentence')\n",
    "            tags     = tf.placeholder(tf.int32, shape=[None], name='tags')\n",
    "            with tf.variable_scope('embedding'):\n",
    "                embedding_var = tf.get_variable('var', shape=(self._vocab_size, self._embedding_dim), dtype=tf.float32, trainable=True)\n",
    "                embeds        = tf.nn.embedding_lookup(embedding_var, sentence)\n",
    "            with tf.variable_scope('transitions'):\n",
    "                transitions_val = np.random.randn(self.tag_size, self.tag_size)\n",
    "                transitions_val[self._tag_to_ix[self.START_TAG], :] = -10000.\n",
    "                transitions_val[:, self._tag_to_ix[self.END_TAG]]   = -10000.\n",
    "                transitions = \\\n",
    "                    tf.get_variable('var', shape=(self.tag_size, self.tag_size), \\\n",
    "                                    initializer=tf.constant_initializer(transitions_val), dtype=tf.float32, trainable=True)\n",
    "            with tf.variable_scope('lstm'):\n",
    "                cell_fw = tf.nn.rnn_cell.BasicLSTMCell(self._hidden_dim)\n",
    "                cell_bw = tf.nn.rnn_cell.BasicLSTMCell(self._hidden_dim)\n",
    "                (outputs_fw, outputs_bw), _ = \\\n",
    "                    tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, tf.expand_dims(embeds, axis=0), dtype=tf.float32)\n",
    "                outputs = tf.concat([outputs_fw, outputs_bw], axis=2)\n",
    "            with tf.variable_scope('hidden2tag'):\n",
    "                feats = tf.layers.dense(tf.squeeze(outputs, axis=[0]), self.tag_size, activation=None)\n",
    "            total_score = self._total_score(feats, transitions)\n",
    "            path_score  = self._path_score(feats, tags, transitions)\n",
    "            loss = total_score - path_score\n",
    "            training_op = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "            best_score, best_path = self._viterbi_decode(feats, transitions)\n",
    "            return sentence, tags, loss, training_op, best_score, best_path\n",
    "    def _path_score(self, feats, tags, transitions):\n",
    "        tags = tf.concat([[self._tag_to_ix[self.START_TAG]], tags], axis=0)\n",
    "        score = tf.reduce_sum(tf.map_fn(lambda x: transitions[x[0], x[1]] + x[2][x[0]], (tags[1:], tags[:-1], feats), dtype=tf.float32))\n",
    "        sum_score = score + transitions[self._tag_to_ix[self.END_TAG], tags[-1]]\n",
    "        return sum_score\n",
    "    def _total_score(self, feats, transitions):\n",
    "        def step(forward, feat):\n",
    "            result = self.log_sum_exp(tf.expand_dims(feat, axis=1) + transitions + forward)\n",
    "            return tf.expand_dims(result, axis=0)\n",
    "        alpha = np.ones((1, self.tag_size), dtype=np.float32) * -10000.\n",
    "        alpha[:, self._tag_to_ix[self.START_TAG]] = 0.\n",
    "        return tf.squeeze(self.log_sum_exp(tf.foldl(step, feats, alpha) + transitions[self._tag_to_ix[self.END_TAG], :]))\n",
    "    def _viterbi_decode(self, feats, transitions):\n",
    "        def step(forward, feat):\n",
    "            next_var = forward + transitions\n",
    "            return tf.reduce_max(next_var, axis=1) + tf.expand_dims(feat, axis=0)\n",
    "        def step_tag(forward):\n",
    "            return tf.argmax(forward + transitions, axis=1, output_type=tf.int32)\n",
    "        def step_path(best_tag_id, backpointer):\n",
    "            return backpointer[best_tag_id]\n",
    "        init_vvars = np.ones((1, self.tag_size), dtype=np.float32) * -10000.\n",
    "        init_vvars[:, self._tag_to_ix[self.START_TAG]] = 0.\n",
    "        forward_vars = tf.scan(step, feats, initializer=init_vvars, back_prop=False)\n",
    "        backpointers = tf.map_fn(step_tag, tf.concat([init_vvars[np.newaxis, :, :], forward_vars[:-1]], axis=0), back_prop=False, dtype=tf.int32)\n",
    "        terminal_var = forward_vars[-1] + transitions[self._tag_to_ix[self.END_TAG]]\n",
    "        best_score   = tf.reduce_max(terminal_var)\n",
    "        best_tag_id  = tf.squeeze(tf.argmax(terminal_var, axis=1, output_type=tf.int32))\n",
    "        reverse_path = tf.scan(step_path, tf.reverse(backpointers, axis=[0]), initializer=best_tag_id, back_prop=False)\n",
    "        assert_op    = tf.Assert(tf.equal(reverse_path[-1], self._tag_to_ix[self.START_TAG]), [reverse_path])\n",
    "        with tf.control_dependencies([assert_op]):\n",
    "            best_path = tf.concat([tf.reverse(reverse_path, axis=[0])[1:], [best_tag_id]], axis=0)\n",
    "        return best_score, best_path\n",
    "    def fit(self, sentence_seq, tags_seq, num_epochs=10, model_file=None):\n",
    "        graph = tf.Graph()\n",
    "        sentence, tags, loss, training_op, _, _ = self._make_graph(graph)\n",
    "        with graph.as_default():\n",
    "            saver = tf.train.Saver() if model_file is not None else None\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(num_epochs):\n",
    "                loss_val = 0\n",
    "                for sentence_val, tags_val in zip(sentence_seq, tags_seq):\n",
    "                    _, loss_val_1 = sess.run([training_op, loss], feed_dict={sentence : sentence_val, tags : tags_val})\n",
    "                    loss_val += loss_val_1\n",
    "                print('epoch [%d/%d], loss: %.3f' % (epoch + 1, num_epochs, loss_val))\n",
    "            if model_file is not None:\n",
    "                model_path = saver.save(sess, model_file)\n",
    "                print('saved model to %s' % model_path)\n",
    "    def predict(self, model_file):\n",
    "        graph = tf.Graph()\n",
    "        sentence, _, _, _, best_score, best_path = self._make_graph(graph)\n",
    "        sess = tf.Session(graph=graph)\n",
    "        with graph.as_default():\n",
    "            tf.train.Saver().restore(sess, model_file)\n",
    "        return \\\n",
    "            lambda sents : [sess.run([best_score, best_path], feed_dict={sentence: sent}) for sent in sents], \\\n",
    "            lambda : sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/10], loss: 19.832\n",
      "epoch [2/10], loss: 13.962\n",
      "epoch [3/10], loss: 10.032\n",
      "epoch [4/10], loss: 7.943\n",
      "epoch [5/10], loss: 6.008\n",
      "epoch [6/10], loss: 3.794\n",
      "epoch [7/10], loss: 2.217\n",
      "epoch [8/10], loss: 1.224\n",
      "epoch [9/10], loss: 0.658\n",
      "epoch [10/10], loss: 0.344\n",
      "saved model to model/lstm-crt-tagger.ckpt\n",
      "INFO:tensorflow:Restoring parameters from model/lstm-crt-tagger.ckpt\n",
      "0\n",
      "\tsentence:  the wall street journal reported today that apple corporation made money\n",
      "\ttarget:  B I I I O O O B I O O\n",
      "\tprediction:  B I I I O O O B I O O\n",
      "\tscore:  45.0344\n",
      "1\n",
      "\tsentence:  georgia tech is a university in georgia\n",
      "\ttarget:  B I O O O O B\n",
      "\tprediction:  B I O O O O B\n",
      "\tscore:  32.1796\n"
     ]
    }
   ],
   "source": [
    "from six import iteritems\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    return np.array([to_ix[w] for w in seq])\n",
    "\n",
    "# Make up some training data\n",
    "training_data = [ (\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ") ]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "ix_to_word = dict([(v, k) for k, v in iteritems(word_to_ix)])\n",
    "            \n",
    "tag_to_ix = { \"B\": 0, \"I\": 1, \"O\": 2, LstmCrfTagger.START_TAG: 3, LstmCrfTagger.END_TAG: 4 }\n",
    "ix_to_tag = dict([(v, k) for k, v in iteritems(tag_to_ix)])\n",
    "\n",
    "model = LstmCrfTagger(vocab_size=len(word_to_ix), tag_to_ix=tag_to_ix)\n",
    "sent_seq, tags_seq = \\\n",
    "    zip(*[(prepare_sequence(sent, word_to_ix), prepare_sequence(tags, tag_to_ix)) for sent, tags in training_data])\n",
    "\n",
    "model.fit(sent_seq, tags_seq, model_file='model/lstm-crt-tagger.ckpt', num_epochs=10)\n",
    "fn, closure = model.predict('model/lstm-crt-tagger.ckpt')\n",
    "try: \n",
    "    scores, paths = zip(*fn(sent_seq))\n",
    "    for i, (sent, tags, score, path) in enumerate(zip(sent_seq, tags_seq, scores, paths)):\n",
    "        print(i)\n",
    "        print('\\tsentence: ', ' '.join(prepare_sequence(sent, ix_to_word)))\n",
    "        print('\\ttarget: ', ' '.join(prepare_sequence(tags, ix_to_tag)))\n",
    "        print('\\tprediction: ', ' '.join(prepare_sequence(path, ix_to_tag)))\n",
    "        print('\\tscore: ', score)\n",
    "finally: closure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
