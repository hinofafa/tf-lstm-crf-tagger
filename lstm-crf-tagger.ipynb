{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmCrfTagger(object):\n",
    "    START_TAG = '<START>'\n",
    "    END_TAG   = '<END>'\n",
    "    @staticmethod\n",
    "    def log_sum_exp(vec):\n",
    "        max_score = tf.reduce_max(vec, axis=1)\n",
    "        return max_score + tf.log(tf.reduce_sum(tf.exp(vec - tf.expand_dims(max_score, axis=1)), axis=1))\n",
    "    @property\n",
    "    def tag_size(self): return len(self._tag_to_ix)    \n",
    "    def __init__(self, vocab_size, tag_to_ix, char_size, embedding_dim=5, hidden_dim=4):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._tag_to_ix = tag_to_ix\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._char_size = char_size\n",
    "    def _make_graph(self, graph):\n",
    "        with graph.as_default():\n",
    "            sentence = tf.placeholder(tf.int32, shape=[None], name='sentence')\n",
    "            tags     = tf.placeholder(tf.int32, shape=[None], name='tags')\n",
    "            words    = tf.placeholder(tf.int32, shape=[None, None], name='words')\n",
    "            words_ln = tf.placeholder(tf.int32, shape=[None], name='words_length')\n",
    "            dropout  = tf.placeholder_with_default(1., shape=(), name='dropout_ratio')\n",
    "            with tf.variable_scope('embedding'):\n",
    "                embedding_words = tf.get_variable('words', shape=(self._vocab_size, self._embedding_dim), dtype=tf.float32, trainable=True)\n",
    "                embeds          = tf.nn.embedding_lookup(embedding_words, sentence, name='word_lookup')\n",
    "            with tf.variable_scope('transitions'):\n",
    "                transitions_val = np.random.randn(self.tag_size, self.tag_size)\n",
    "                transitions_val[self._tag_to_ix[self.START_TAG], :] = -10000.\n",
    "                transitions_val[:, self._tag_to_ix[self.END_TAG]]   = -10000.\n",
    "                transitions = \\\n",
    "                    tf.get_variable('matrix', shape=(self.tag_size, self.tag_size), \\\n",
    "                                    initializer=tf.constant_initializer(transitions_val), dtype=tf.float32, trainable=True)\n",
    "            with tf.variable_scope('words_lstm'):\n",
    "                cell_fw = \\\n",
    "                    tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(self._hidden_dim), input_keep_prob=dropout)\n",
    "                cell_bw = \\\n",
    "                    tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(self._hidden_dim), input_keep_prob=dropout)\n",
    "                (outputs_fw, outputs_bw), _ = \\\n",
    "                    tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, tf.expand_dims(embeds, axis=0), dtype=tf.float32)\n",
    "                words_outputs = tf.squeeze(tf.concat([outputs_fw, outputs_bw], axis=2), axis=[0])\n",
    "            with tf.variable_scope('embedding'):\n",
    "                embedding_chars = tf.get_variable('chars', shape=(self._char_size + 1, self._embedding_dim), dtype=tf.float32, trainable=True)\n",
    "                embeds          = tf.nn.embedding_lookup(embedding_chars, words, name='char_lookup')\n",
    "            with tf.variable_scope('chars_lstm'):\n",
    "                cell_fw = \\\n",
    "                    tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(self._hidden_dim), input_keep_prob=dropout)\n",
    "                cell_bw = \\\n",
    "                    tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(self._hidden_dim), input_keep_prob=dropout)\n",
    "                _, ((_, output_state_fw), (_, output_state_bw)) = \\\n",
    "                    tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, embeds, sequence_length=words_ln, dtype=tf.float32)\n",
    "                chars_outputs = tf.concat([output_state_fw, output_state_bw], axis=1)\n",
    "            outputs = tf.concat([words_outputs, chars_outputs], axis=1)\n",
    "            with tf.variable_scope('hidden2tag'):\n",
    "                feats = tf.layers.dense(outputs, self.tag_size, activation=None)\n",
    "            total_score = self._total_score(feats, transitions)\n",
    "            path_score  = self._path_score(feats, tags, transitions)\n",
    "            loss = total_score - path_score\n",
    "            training_op = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "            best_score, best_path = self._viterbi_decode(feats, transitions)\n",
    "            return dropout, sentence, tags, words, words_ln, loss, training_op, best_score, best_path\n",
    "    def _path_score(self, feats, tags, transitions):\n",
    "        tags = tf.concat([[self._tag_to_ix[self.START_TAG]], tags], axis=0)\n",
    "        score = tf.reduce_sum(tf.map_fn(lambda x: transitions[x[0], x[1]] + x[2][x[0]], (tags[1:], tags[:-1], feats), dtype=tf.float32))\n",
    "        sum_score = score + transitions[self._tag_to_ix[self.END_TAG], tags[-1]]\n",
    "        return sum_score\n",
    "    def _total_score(self, feats, transitions):\n",
    "        def step(forward, feat):\n",
    "            result = self.log_sum_exp(tf.expand_dims(feat, axis=1) + transitions + forward)\n",
    "            return tf.expand_dims(result, axis=0)\n",
    "        alpha = np.ones((1, self.tag_size), dtype=np.float32) * -10000.\n",
    "        alpha[:, self._tag_to_ix[self.START_TAG]] = 0.\n",
    "        return tf.squeeze(self.log_sum_exp(tf.foldl(step, feats, alpha) + transitions[self._tag_to_ix[self.END_TAG], :]))\n",
    "    def _viterbi_decode(self, feats, transitions):\n",
    "        def step(forward, feat):\n",
    "            next_var = forward + transitions\n",
    "            return tf.reduce_max(next_var, axis=1) + tf.expand_dims(feat, axis=0)\n",
    "        def step_tag(forward):\n",
    "            return tf.argmax(forward + transitions, axis=1, output_type=tf.int32)\n",
    "        def step_path(best_tag_id, backpointer):\n",
    "            return backpointer[best_tag_id]\n",
    "        init_vvars = np.ones((1, self.tag_size), dtype=np.float32) * -10000.\n",
    "        init_vvars[:, self._tag_to_ix[self.START_TAG]] = 0.\n",
    "        forward_vars = tf.scan(step, feats, initializer=init_vvars, back_prop=False)\n",
    "        backpointers = tf.map_fn(step_tag, tf.concat([init_vvars[np.newaxis, :, :], forward_vars[:-1]], axis=0), back_prop=False, dtype=tf.int32)\n",
    "        terminal_var = forward_vars[-1] + transitions[self._tag_to_ix[self.END_TAG]]\n",
    "        best_score   = tf.reduce_max(terminal_var)\n",
    "        best_tag_id  = tf.squeeze(tf.argmax(terminal_var, axis=1, output_type=tf.int32))\n",
    "        reverse_path = tf.scan(step_path, tf.reverse(backpointers, axis=[0]), initializer=best_tag_id, back_prop=False)\n",
    "        assert_op    = tf.Assert(tf.equal(reverse_path[-1], self._tag_to_ix[self.START_TAG]), [reverse_path])\n",
    "        with tf.control_dependencies([assert_op]):\n",
    "            best_path = tf.concat([tf.reverse(reverse_path, axis=[0])[1:], [best_tag_id]], axis=0)\n",
    "        return best_score, best_path\n",
    "    def fit(self, sentence_seq, tags_seq, words_seq, words_len_seq, num_epochs=10, model_file=None, drop_out=.5):\n",
    "        graph = tf.Graph()\n",
    "        dropout, sentence, tags, words, words_ln, loss, training_op, _, _ = self._make_graph(graph)\n",
    "        with graph.as_default():\n",
    "            saver = tf.train.Saver() if model_file is not None else None\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(num_epochs):\n",
    "                loss_val = 0\n",
    "                for sentence_val, tags_val, words_val, words_len_val in zip(sentence_seq, tags_seq, words_seq, words_len_seq):\n",
    "                    feed_dict = { dropout: drop_out, sentence : sentence_val, tags : tags_val, words : words_val + 1, words_ln : words_len_val }\n",
    "                    _, loss_val_1 = sess.run([training_op, loss], feed_dict=feed_dict)\n",
    "                    loss_val += loss_val_1\n",
    "                print('epoch [%d/%d], loss: %.3f' % (epoch + 1, num_epochs, loss_val))\n",
    "            if model_file is not None:\n",
    "                model_path = saver.save(sess, model_file)\n",
    "                print('saved model to %s' % model_path)\n",
    "    def predict(self, model_file):\n",
    "        graph = tf.Graph()\n",
    "        _, sentence, _, words, words_len, _, _, best_score, best_path = self._make_graph(graph)\n",
    "        sess = tf.Session(graph=graph)\n",
    "        with graph.as_default():\n",
    "            tf.train.Saver().restore(sess, model_file)\n",
    "        return \\\n",
    "            lambda sents, ws, wls : [sess.run([best_score, best_path], feed_dict={sentence: sent, words : w + 1, words_len : wl}) for sent, w, wl in zip(sents, ws, wls)], \\\n",
    "            lambda : sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/10], loss: 21.324\n",
      "epoch [2/10], loss: 13.394\n",
      "epoch [3/10], loss: 11.670\n",
      "epoch [4/10], loss: 8.206\n",
      "epoch [5/10], loss: 5.775\n",
      "epoch [6/10], loss: 4.223\n",
      "epoch [7/10], loss: 2.475\n",
      "epoch [8/10], loss: 2.080\n",
      "epoch [9/10], loss: 1.824\n",
      "epoch [10/10], loss: 3.460\n",
      "saved model to model/lstm-crf-tagger.ckpt\n",
      "INFO:tensorflow:Restoring parameters from model/lstm-crf-tagger.ckpt\n",
      "0\n",
      "\tsentence:  the wall street journal reported today that apple corporation made money\n",
      "\ttarget:  B I I I O O O B I O O\n",
      "\tprediction:  B I I I I O O B I O O\n",
      "\tscore:  36.250656\n",
      "1\n",
      "\tsentence:  georgia tech is a university in georgia\n",
      "\ttarget:  B I O O O O B\n",
      "\tprediction:  B I O O O O B\n",
      "\tscore:  30.8023\n"
     ]
    }
   ],
   "source": [
    "from six import iteritems\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    return np.array([to_ix[w] for w in seq])\n",
    "\n",
    "# Make up some training data\n",
    "training_data = [ (\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ") ]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "ix_to_word = dict([(v, k) for k, v in iteritems(word_to_ix)])\n",
    "\n",
    "char_to_ix = {}\n",
    "for word in word_to_ix.keys():\n",
    "    for char in word:\n",
    "        if char not in char_to_ix:\n",
    "            char_to_ix[char] = len(char_to_ix)\n",
    "ix_to_char = dict([(v, k) for k, v in iteritems(char_to_ix)])\n",
    "\n",
    "tag_to_ix = { \"B\": 0, \"I\": 1, \"O\": 2, LstmCrfTagger.START_TAG: 3, LstmCrfTagger.END_TAG: 4 }\n",
    "ix_to_tag = dict([(v, k) for k, v in iteritems(tag_to_ix)])\n",
    "\n",
    "def prepare_input(sentence, tags):\n",
    "    sent_seq = prepare_sequence(sentence, word_to_ix)\n",
    "    tags_seq = prepare_sequence(tags, tag_to_ix)\n",
    "    word_seq = \\\n",
    "        tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [prepare_sequence(word, char_to_ix) for word in sentence], \n",
    "            padding='post', \n",
    "            value=-1)\n",
    "    word_len_seq = \\\n",
    "        np.apply_along_axis(\n",
    "            lambda seq: next(i for i, j in enumerate(seq) if j < 0), \n",
    "            axis=1, \n",
    "            arr=np.c_[word_seq, np.ones((word_seq.shape[0], 1)) * -1])\n",
    "    return sent_seq, tags_seq, word_seq, np.squeeze(word_len_seq)\n",
    "\n",
    "model = LstmCrfTagger(vocab_size=len(word_to_ix), tag_to_ix=tag_to_ix, char_size=len(char_to_ix))\n",
    "sent_seq, tags_seq, word_seq, word_len_seq = \\\n",
    "    zip(*[prepare_input(sent, tags) for sent, tags in training_data])\n",
    "model.fit(sent_seq, tags_seq, word_seq, word_len_seq, model_file='model/lstm-crf-tagger.ckpt', num_epochs=10)\n",
    "\n",
    "fn, closure = model.predict('model/lstm-crf-tagger.ckpt')\n",
    "try: \n",
    "    scores, paths = zip(*fn(sent_seq, word_seq, word_len_seq))\n",
    "    for i, (sent, tags, score, path) in enumerate(zip(sent_seq, tags_seq, scores, paths)):\n",
    "        print(i)\n",
    "        print('\\tsentence: ', ' '.join(prepare_sequence(sent, ix_to_word)))\n",
    "        print('\\ttarget: ', ' '.join(prepare_sequence(tags, ix_to_tag)))\n",
    "        print('\\tprediction: ', ' '.join(prepare_sequence(path, ix_to_tag)))\n",
    "        print('\\tscore: ', score)\n",
    "finally: closure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
